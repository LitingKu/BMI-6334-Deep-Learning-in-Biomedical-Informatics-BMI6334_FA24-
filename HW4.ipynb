{"cells":[{"cell_type":"markdown","metadata":{"id":"TW-ri0ae1l1Y"},"source":["### Import packages and get access to the training text file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOsPipPkq4r3","outputId":"91cc6a89-25ec-4f32-809a-a9c1485ff179","executionInfo":{"status":"ok","timestamp":1728862251796,"user_tz":300,"elapsed":984,"user":{"displayName":"Li-Ting Ku","userId":"07885881455222019101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/HW4\n"]}],"source":["### For colab usage\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/My Drive/Colab Notebooks/HW4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVqj46rcPfVD","outputId":"c8b9a62f-c71d-446e-9725-c7ae9348c8c3","executionInfo":{"status":"ok","timestamp":1728862265227,"user_tz":300,"elapsed":4409,"user":{"displayName":"Li-Ting Ku","userId":"07885881455222019101"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unidecode\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.8\n"]}],"source":["!pip install unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWg80xxQycC9"},"outputs":[],"source":["import unidecode\n","import string\n","import random\n","import time\n","import math\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import argparse\n","import os\n","\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_vYppyYIpvQG"},"outputs":[],"source":["### helpers.py\n","\n","def read_file(filename):\n","    file = unidecode.unidecode(open(filename).read())\n","    return file, len(file)\n","\n","filename = 'medline.0.txt'\n","file, file_len = read_file(filename)"]},{"cell_type":"markdown","metadata":{"id":"u7eklqsO1xv9"},"source":["### Functions to create the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V-y0OCQygyF"},"outputs":[],"source":["### train.py\n","### corrected one mistake in train of using cuda\n","### corrected one mistake in the last line of train: previous code is loss.data[0]\n","### which causes error, so changed to loss\n","\n","def random_training_set(chunk_len, batch_size):\n","    inp = torch.LongTensor(batch_size, chunk_len)\n","    target = torch.LongTensor(batch_size, chunk_len)\n","    for bi in range(batch_size):\n","        start_index = random.randint(0, file_len - chunk_len)\n","        end_index = start_index + chunk_len + 1\n","        chunk = file[start_index:end_index]\n","        inp[bi] = char_tensor(chunk[:-1])\n","        target[bi] = char_tensor(chunk[1:])\n","    inp = Variable(inp)\n","    target = Variable(target)\n","    if cuda:\n","        inp = inp.cuda()\n","        target = target.cuda()\n","    return inp, target\n","\n","def train(inp, target):\n","    hidden = decoder.init_hidden(batch_size)\n","\n","    if cuda:\n","        #Can't convert hidden to cuda because hidden is a tuple of tensor, not tensor.\n","        #Need to convert it to list, then convert each of the elements to cuda,\n","        #then convert back to a tuple.\n","        hidden = list(hidden)\n","        hidden[0] = hidden[0].cuda()\n","        hidden[1] = hidden[1].cuda()\n","        hidden = tuple(hidden)\n","        #hidden = hidden.cuda()\n","\n","    decoder.zero_grad()\n","    loss = 0\n","\n","    for c in range(chunk_len):\n","        output, hidden = decoder(inp[:,c], hidden)\n","        loss += criterion(output.view(batch_size, -1), target[:,c])\n","\n","    loss.backward()\n","    decoder_optimizer.step()\n","\n","    return loss / chunk_len\n","\n","def save():\n","    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n","    torch.save(decoder, save_filename)\n","    print('Saved as %s' % save_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwO9Zxdjyg3v"},"outputs":[],"source":["### model.py\n","\n","class CharRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, model=\"gru\", n_layers=1):\n","        super(CharRNN, self).__init__()\n","        self.model = model.lower()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        self.encoder = nn.Embedding(input_size, hidden_size)\n","        if self.model == \"gru\":\n","            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n","        elif self.model == \"lstm\":\n","            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n","            #self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers, bias=False) #try to set bias as False\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input, hidden):\n","        batch_size = input.size(0)\n","        encoded = self.encoder(input)\n","        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n","        output = self.decoder(output.view(batch_size, -1))\n","        return output, hidden\n","\n","    def forward2(self, input, hidden):\n","        encoded = self.encoder(input.view(1, -1))\n","        output, hidden = self.rnn(encoded.view(1, 1, -1), hidden)\n","        output = self.decoder(output.view(1, -1))\n","        return output, hidden\n","\n","    def init_hidden(self, batch_size):\n","        if self.model == \"lstm\":\n","            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n","                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n","        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UpdqrjLy99W"},"outputs":[],"source":["### generate.py\n","### corrected one mistake of using cuda, the same as in train\n","\n","def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False):\n","    hidden = decoder.init_hidden(1)\n","    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n","\n","    if cuda:\n","      # using LSTM: tuple tensors\n","        hidden = list(hidden)\n","        hidden[0] = hidden[0].cuda()\n","        hidden[1] = hidden[1].cuda()\n","        hidden = tuple(hidden)\n","        #hidden = hidden.cuda()\n","        prime_input = prime_input.cuda()\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        _, hidden = decoder.forward2(prime_input[:,p], hidden)\n","\n","    inp = prime_input[:,-1]\n","\n","    for p in range(predict_len):\n","        output, hidden = decoder.forward2(inp, hidden)\n","\n","        # Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","\n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n","        if cuda:\n","            inp = inp.cuda()\n","\n","    return predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYW6jisB2lQw"},"outputs":[],"source":["### healpers.py\n","\n","def time_since(since):\n","    s = time.time() - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string)).long()\n","    for c in range(len(string)):\n","        try:\n","            tensor[c] = all_characters.index(string[c])\n","        except:\n","            continue\n","    return tensor"]},{"cell_type":"markdown","metadata":{"id":"laoPc1XR28hD"},"source":["### Training and Generate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkX3AKM13bbg"},"outputs":[],"source":["all_characters = string.printable\n","n_characters = len(all_characters)\n","n_layers = 4\n","n_epochs = 2000\n","\n","hidden_size = 100\n","batch_size = 100\n","learning_rate = 0.01\n","model = \"lstm\"\n","\n","chunk_len = 500\n","print_every = 100\n","cuda = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8bQrTjjyg6P"},"outputs":[],"source":["decoder = CharRNN(\n","    n_characters,\n","    hidden_size,\n","    n_characters,\n","    model=model,\n","    n_layers=n_layers,\n",")\n","\n","if cuda:\n","    decoder.cuda()\n","\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","loss_avg = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdzYuQ45yg_Q","outputId":"75467d37-2ded-49d9-d0c9-a4f4a3fb10f0","scrolled":true,"executionInfo":{"status":"ok","timestamp":1728867141729,"user_tz":300,"elapsed":4851907,"user":{"displayName":"Li-Ting Ku","userId":"07885881455222019101"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["  5%|▍         | 99/2000 [03:01<1:00:40,  1.92s/it]"]},{"output_type":"stream","name":"stdout","text":["[3m 3s (100 5%) 3.2963]\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 100/2000 [03:02<1:00:36,  1.91s/it]"]},{"output_type":"stream","name":"stdout","text":["Who r ut n tP89tde\n","0teg2(0An9a2 Aso Mc/nitc s cec ter \n",":ay,.Ho0Rta S9(3nt  r rt o1AAet Blass wAUaT2ge( \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 200/2000 [06:00<50:41,  1.69s/it]"]},{"output_type":"stream","name":"stdout","text":["[6m 1s (200 10%) 2.3783]\n","Wh  \n","      al anitoitoenmes\n","      nhich sa  os b repasaath (Aemune oleff prr toec the nhor we avhad\n","   \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 300/2000 [09:01<52:07,  1.84s/it]"]},{"output_type":"stream","name":"stdout","text":["[9m 2s (300 15%) 1.7867]\n","Wh, Ninal Sespiomed\n","MH  - Tors S Kergesdiry Sales)\n","RB  - CLA pucences' sustent dew the insed wannic an \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 400/2000 [11:58<44:47,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["[11m 59s (400 20%) 1.5255]\n","Whinicabecing protein of R602 R=0.8764). Croen sars)\n","SB  - TAS Protein 1/pran Intiin Union Prostrestic \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 500/2000 [14:58<46:53,  1.88s/it]"]},{"output_type":"stream","name":"stdout","text":["[14m 58s (500 25%) 1.3608]\n","Whiraymmy.\n","AN  - Eemology\n","MH  - Reproteocediology\n","MH  - Ccurunosis/*putoratics\n","MH  - Dulthitional Youm \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 600/2000 [17:56<39:59,  1.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[17m 57s (600 30%) 1.2818]\n","Whid do cells in preclin. POT-heng\n","PT  - Journal Article\n","PT  - Research Subter\n","DEP - 20120928\n","PL  - Un \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 35%|███▌      | 700/2000 [20:55<41:51,  1.93s/it]"]},{"output_type":"stream","name":"stdout","text":["[20m 56s (700 35%) 1.1722]\n","Whyl\n","AU  - Elstrono-Hate\n","AU  - Wagem T\n","FIR - Liren, Erlar\n","AU  - Calder I\n","FAU - Geni, Nison\n","AU  - Mengk \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 800/2000 [23:53<33:38,  1.68s/it]"]},{"output_type":"stream","name":"stdout","text":["[23m 53s (800 40%) 1.1225]\n","Whe\n","AU  - Sunnebatimene A\n","AD  - Tumor-Nometral Article\n","PT  - Research Support, Ebtity\n","MH  - Humans\n","MH  \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 45%|████▌     | 900/2000 [26:52<35:49,  1.95s/it]"]},{"output_type":"stream","name":"stdout","text":["[26m 53s (900 45%) 1.1668]\n","Whophaneted breast need\n","      havuor the that greate receftional and epithic specific vollad-manientin \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 1000/2000 [29:50<27:50,  1.67s/it]"]},{"output_type":"stream","name":"stdout","text":["[29m 51s (1000 50%) 1.1690]\n","Whe Hemwer, Surgical Surgery\n","MH  - Antineoplastic Biology\n","MH  - Cell Oncology\n","MH  - Cell Englanding As \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▌    | 1100/2000 [32:52<25:59,  1.73s/it]"]},{"output_type":"stream","name":"stdout","text":["[32m 53s (1100 55%) 1.1568]\n","Whes and surgerials (ORD) beterted\n","      progressor resity, groups, tragal ATGPF total of becquatic br \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 1200/2000 [35:53<27:52,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["[35m 54s (1200 60%) 1.0975]\n","Whieth Factors\n","PMC - PMC4159832\n","EDAT- 2012/09/25 06:00\n","MHDA- 2013/03/29 06:000\n","\n","RN  - 0 (TPK)) for EBF \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 65%|██████▌   | 1300/2000 [38:54<19:22,  1.66s/it]"]},{"output_type":"stream","name":"stdout","text":["[38m 55s (1300 65%) 1.0956]\n","Wh the biological metastatic cancer complications \n","      and adsors are suppressor that the PE/CEREDE: \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 1400/2000 [41:53<17:44,  1.77s/it]"]},{"output_type":"stream","name":"stdout","text":["[41m 54s (1400 70%) 1.0361]\n","Whe follemic, after was\n","      metastasis and on the prevalence of learoletammation in oxcology was\n","    \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 75%|███████▌  | 1500/2000 [44:52<15:43,  1.89s/it]"]},{"output_type":"stream","name":"stdout","text":["[44m 53s (1500 75%) 1.0814]\n","Whecang chemotherapy for the TDC/CR cells by\n","      enacimulating at proncology localizing pnected tumo \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 1600/2000 [47:54<11:38,  1.75s/it]"]},{"output_type":"stream","name":"stdout","text":["[47m 54s (1600 80%) 1.0579]\n","Whokin.\n","FAU - Yuide, Cither\n","AU  - Henola H\n","AD  - Discostic of Progence Totting, Ster. 01106-9614 (Elec \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 1700/2000 [50:56<09:13,  1.85s/it]"]},{"output_type":"stream","name":"stdout","text":["[50m 56s (1700 85%) 0.9978]\n","Whad 710 could intereftrutic and\n","      significant review for extter, as not mRN gene screening analys \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 1800/2000 [53:55<06:25,  1.93s/it]"]},{"output_type":"stream","name":"stdout","text":["[53m 56s (1800 90%) 1.1064]\n","Whalt\n","      cancer sites. The regulation a cells to acquantic dependently were\n","      the a chemical ri \n","\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▌| 1900/2000 [56:54<03:10,  1.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[56m 55s (1900 95%) 1.0381]\n","Wh M-Mille Cells\n","JID - 101225837\n","RN  - 0 (Antineoplastic Agents)\n","RN  - 0 (RNA, Medicine, Immunokinetic \n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2000/2000 [1:20:50<00:00,  2.43s/it]"]},{"output_type":"stream","name":"stdout","text":["[80m 51s (2000 100%) 0.9141]\n","Whased There platin & cell lesion more and specialite and\n","      this volume at developing tumor cit bl \n","\n","Saving...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Saved as medline.0.pt\n"]}],"source":["for epoch in tqdm(range(1, n_epochs + 1)):\n","    loss = train(*random_training_set(chunk_len, batch_size))\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n","        print(generate(decoder, 'Wh', 100, cuda=cuda), '\\n')\n","\n","print(\"Saving...\")\n","save()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1YNYqdcyhEA","outputId":"87830134-671e-4435-e09d-9a940436ce59","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1728859281103,"user_tz":300,"elapsed":1945,"user":{"displayName":"Li-Ting Ku","userId":"15140832011357554359"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-37-87efb78ee750>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n"]},{"output_type":"execute_result","data":{"text/plain":["\"PMID- Non-Sp)m Sci.490-512. doi orged head tissues. Activator cells.\\nPG  - 32-68\\nLID - 10.1016/j.dpp.202200978 [doi]\\nAB  - BACKGROUND: Patussay. The locology, Facing and exama and significant internal better in\\n      concence analysis of the gene powen of 62.7%, p = 0.591 % (Darraielzy\\nAU  - Kim Hormonn TS\\nAD  - Department of Cancer and France. Amiliation of the European Social Several Amages of the\\n      University Institute of Biology de Human Pharmacysig. We University a\\n      regimatory as selective status and differentially was a positive\\n      improved eachimetins have developments were embraint \\n      constude are results were : induction and studies rearran pancreatic cancer carcinoma\\n      staged seculation on breast percutoned by our patient death) = 0.79 months\\n      those reactivity.\\nFAU - Poto, Rodenanica\\nAU  - Chun MY\\nFAU - Tang, Perpavitie\\nAU  - Arettol G\\nLA  - eng\\nPT  - Journal Article\\nPT  - Research Support, Non-U.S. Gov't\\nPT  - Review\\nPL  - United States\\nTA  - J All Cyst Med\\nJT  - J Physich Facine of Health-social\\n      mutations and control for prostate cancer cells to anti-cancer treatment with reported\\n      of prescride and natural vitural oncology and increased to the subjects and\\n      kindument colorectal cancer cells may survival muditonal factor of consumpting on\\n      meta-analysis. Chemoradione \\n      of the treatment were consideration of colon cancer (CD) according to a relationship. Human \\n      of predict sedia aged with advantages, this cohort hundurence for colorectal cancer \\n      frequent proposed de durint conclusion of response for is\\n      suggested to \\n      assisted that 14.7%, KRAS with replay were assessed and this by represents has for the remaining\\n      potential and highests and has considerations in a review related in patients which\\n      increased carcinoma be assigned clinical cancer continue with and hyperplastic factors\\n      and underwent survival and formates database cross-sectional factor and\\n      conformed the \""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}],"source":["# using 2 layers\n","generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)"]},{"cell_type":"code","source":["# using 4 layers\n","generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"WdgLk7F3gjHe","executionInfo":{"status":"ok","timestamp":1728867150065,"user_tz":300,"elapsed":1752,"user":{"displayName":"Li-Ting Ku","userId":"07885881455222019101"}},"outputId":"2839beb1-bfaf-4890-b14e-3b046c9b8f9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-15-059a012d4bcf>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n"]},{"output_type":"execute_result","data":{"text/plain":["'PMID- 23009514\\nOWN - NLM\\nSTAT- MEDLINE\\nDCOM- 20130413\\nLR  - 20170220\\nIS  - 1742-5100 (Electronic)\\nIS  - 1679-1947 (Linking)\\nVI  - 23\\nIP  - 1\\nDP  - 2013 Jan 23\\nTI  - The on lice and certicipate that breast cancer temosis to classified\\n      to statisticity stent risk and analyses, involvement of metastases in response of \\n      a valuable cancer in the significantly and contributeal study secitant and reduction, the rall,\\n      cancer conventional prostate status.\\nPG  - 839-74\\nLID - 10.1002/cncr.2330 [doi]\\nAB  - Chronic based the prostate cancer activation and anticancer factors to Jatude to\\n      temiate to the arogen these immunohistochemical sensitives were genes for the nanostatistically\\n      risk of 27 patients with a sequencing therapy of the completed proteins, differention-associated\\n      protein biomarkers. Eureton perithents with has estelners.\\nFAU - Forecands Patrunn-Ah. Crind\\nAU  - Kaman Jan WJ\\nLA  - eng\\nPT  - Journal Article\\nDEP - 20120920\\nPL  - United States\\nTA  - Gastroenterology\\nJT  - Journal of several metastatic cancer was nutrition of\\n      promotes staging prostate cancer cells. Althich high cervical cancer findings in mortality of\\n      larged including key impact of recurrence and [ys to further cyclin status,\\n      caron of the anti-(10 toclinical complexit) negative respect\\n      lesions could to intexin (GSPTA steroid mality, include) and need to tumor immunomes heterogene; when\\n      score by the Cell line compared post-state pathublication, the role and standard,\\n      the exability of both relative GVSATS and Line-controls models with addition,\\n      showed by patients with ICSs and and consider and concurrent single-Godes with the diagnosis of BEBM\\n      significant deer the reRekinty extcomers. In cells, negative considered. After the approaches, the as\\n      stem therapeutic consent cytosine cancer. We prong the positive breast the diagnosed alger\\n      cervical-dependent patient cancer expression of patients and these cell repural nicrob'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0VymNRnyhMq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728867158829,"user_tz":300,"elapsed":5260,"user":{"displayName":"Li-Ting Ku","userId":"07885881455222019101"}},"outputId":"6fdb58f4-a4c9-4161-99be-8a23d1109303"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-b5ff4af01f7f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fake_paragraph_1 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n","<ipython-input-16-b5ff4af01f7f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fake_paragraph_2 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n","<ipython-input-16-b5ff4af01f7f>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  fake_paragraph_3 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n"]}],"source":["fake_paragraph_1 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n","fake_paragraph_2 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)\n","fake_paragraph_3 = generate(decoder=torch.load(\"medline.0.pt\"), prime_str=\"PMID\", predict_len=2000, temperature=0.8, cuda=True)"]},{"cell_type":"code","source":["real_paragraph = file[:2000]\n","import random\n","paragraphs = [fake_paragraph_1, fake_paragraph_2, fake_paragraph_3, real_paragraph]\n","random.shuffle(paragraphs)\n","\n","# Step 5: Write the shuffled paragraphs to a file\n","with open('Li-TingKu4.MedLine.Sample.txt', 'w') as f:\n","    for i, paragraph in enumerate(paragraphs):\n","        f.write(f\"Paragraph {i + 1}:\\n\")\n","        f.write(paragraph + \"\\n\\n\")"],"metadata":{"id":"amKTHls4mvpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Summary\n","\n","Instead of using the command line, I made them into the python script together so I could use them more clearly. I first found that there are two forward method in the CharRNN model, so I made the `forward` into the training process with batches of sequences, whereas `forward2` be used into the generating process for sequential character generation. Then I select the LSTM model which is more complex then GRU because the pubmed abstract seems to be a complex text. In the training process, I want to try wth different layers to increase the model capacity and to learn complex patterns. I used 2 layers with 2000 epoch and 4 layers with 2000 epoch to see if there will be some difference, but I feel like the two result gave me similar structure of the pubmed abstract output, and when reading the context in the two results, I think using 4 layers generate more reasonable context compared to 2 layers.\n"],"metadata":{"id":"xG_E6vfeEB0M"}},{"cell_type":"code","source":[],"metadata":{"id":"iT5tasrUzfta"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1VsPhTEMB2S6FS2n5eSuUCjTKV0bnV_qM","timestamp":1728867299006}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}